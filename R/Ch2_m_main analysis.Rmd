---
title: "Barred Owl Individual Vocalization using Passive Acoustic Monitoring"
author: "Sunny Tseng"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Library & functions

Load necessary libraries: 
```{r, message = FALSE, warning = FALSE}
library(here)
library(tidyverse)
library(lubridate)
library(reshape2)
library(sf)
library(leaflet)
library(ggmap)
library(ggspatial)
library(ggord)

library(factoextra)
library(ggbiplot)
library(corrplot)
library(psych)
library(caret)
library(MASS)
library(klaR)
library(heplots)
```


Also do this to avoid name conflict:
```{r}
here <- here::here
select <- dplyr::select
filter <- dplyr::filter
summarize <- dplyr::summarize
group_by <- dplyr::group_by
rename <- dplyr::rename
```


Load personal functions:
```{r}
source(here("R", "Ch2_1_qualified_recording_list.R"))
source(here("R", "Ch2_2_move_recordings_to_folder.R"))
source(here("R", "Ch2_3_parameters_from_raventable.R"))
source(here("R", "Ch2_4_lda_n_fold_validation.R"))
```


## Data import & data cleaning

The original output from BirdNET, filtered by species == Barred Owl: 
```{r, message = FALSE}
data_birdnet <- read_csv(here("data", "barred_owl_recordings_list.csv"))

data_birdnet_clean <- data_birdnet %>%
  filter(year == 2021) %>%
  drop_na() %>%
  mutate(date = ymd(paste0(year, month, day)),
         hour = str_sub(recording, start = 10, end = 11),
         Julian = yday(date), 
         day_period = cut(Julian, 
                          breaks = c(40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150)))
```


The song dataset after manual extraction of features. Only keep the sites with more than 9 calls:
```{r, message = FALSE}
data <- read_csv(here("data", "barred_owl_recordings_list_parameters_2.csv"))

data_clean <- data %>%
  filter_all(all_vars(. != 0)) %>%
  mutate(site = str_sub(sound.files, start = 1, end = 5),
         year = str_sub(sound.files, start = 7, end = 10),
         month = str_sub(sound.files, start = 11, end = 12),
         day = str_sub(sound.files, start = 13, end = 14),
         unique_ID = seq(from = 1, to = nrow(.)),
         individual = paste0(year, "_", site),
         date = ymd(paste0(year, month, day)))

sites <- data_clean %>%
  group_by(site) %>%
  summarize(n = n()) %>%
  filter(n >= 9) %>%
  pull(site)

data_clean_1 <- data_clean %>%
  filter(site %in% sites)
```


Location of the 66 ARU sites:
```{r, message = FALSE}
data_location <- read_csv(here("data", "JPRF_all_sites.csv"))

data_location_clean <- data_location %>%
  rename("site" = Name, "longitude" = X, "latitude" = Y) %>%
  select(site, longitude, latitude) %>%
  mutate(group = str_extract(site, pattern = ".+(?=\\_)"))
```


## Data visualization

Temporal distribution of the Barred Owl calls, after filtering out sites with fewer than 9 calls: 
```{r, message = FALSE}
temporal_songs <- data_clean_1 %>%
  mutate(site = str_sub(site, start = 4, end = 5)) %>%
  group_by(date, site) %>%
  summarize(n_songs = n()) %>%
  ungroup() %>%
  ggplot(aes(x = date, y = site, fill = n_songs)) +
    geom_point(shape = 22, size = 7) +
    scale_fill_gradient(low = "mistyrose", high = "red") +
    theme_bw() +
    theme(legend.position = c(0.99, 0.05), 
          legend.justification = c("right", "bottom"),
          legend.box.just = "right",
          axis.title = element_text(size = 14),
          axis.text = element_text(size = 12),
          axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
          axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) +
    labs(fill = "# of calls", x = "Month", y = "Site")
         #title = "# of BDOW calls per site per day")

temporal_songs

ggsave(plot = temporal_songs, 
       filename = here("docs", "figures", "temporal_songs.png"), 
       height = 12, 
       width = 20, 
       units = "cm",
       dpi = 300)
```


Spatial distribution of the ARU sites and Barred Owl calls:
```{r}
JPRF_sf <- full_join(data_location_clean, data_clean_1 %>% group_by(site) %>% summarize(n_songs = n()),
                     by = "site", replace_na) %>%
  replace_na(list(n_songs = 0)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  ggplot(data = .) +
    geom_sf(aes(colour = group), shape = 3, stroke = 1.5, size = 3) +
    geom_sf(aes(size = n_songs), shape = 1, stroke = 2, data = . %>% filter(n_songs != 0)) +
    annotation_scale(location = "bl", width_hint = 0.4) +
    annotation_north_arrow(location = "bl", which_north = "true",
                           pad_x = unit(0.2, "in"), pad_y = unit(0.3, "in"),
                           style = north_arrow_fancy_orienteering) +
    coord_sf(xlim = c(-124.7, -124.1), ylim = c(54.55, 54.85)) +
    theme_bw()

JPRF_sf

```


Spatial distribution of the ARU sites using package leavlet:
```{r, eval = FALSE}
field_sites <- JPRF_sf %>%
  rename("site_name" = site)

leaflet() %>%
  setView(lng = -123.5, lat = 52, zoom = 10) %>%
  addProviderTiles("Stamen") %>%
  addCircleMarkers(
    data = field_sites,
    radius = 5,
    color = "black",
    stroke = FALSE,
    fillOpacity = 0.8,
    popup = paste("Site name: ", field_sites$site_name)
  )
```


## Predictor variables 

Examine the variables performance by looking at the CV:
```{r}
statistics <- data_clean_1 %>%
  select(site, sl:meandom_8) %>%
  pivot_longer(cols = c(-site), names_to = "variable", values_to = "value") %>%
  group_nest(variable) %>%
  mutate(mean = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% mean()),
         sd = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% sd()),
         min = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% min()),
         max = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% max()),
         CV_group = map_dbl(.x = data, .f =~ sd(.x$value)/mean(.x$value)*100),
         CV_individual = map_dbl(.x = data, .f =~ .x %>% 
                                   group_by(site) %>%
                                   dplyr::summarise(CV = sd(value)/mean(value)) %>%
                                   pull(CV) %>%
                                   mean() *100),
         ANOVA = map_dbl(.x = data, .f =~ .x %>%
                           lm(formula = value ~ site) %>%
                           anova() %>%
                           .$`Pr(>F)` %>%
                           .[1])) %>%
  select(-data)

statistics
```

Get the descriptive statistics by site
```{r}
stats_by_station <- data_clean_1 %>%
  select(site, sl:meandom_8) %>%
  pivot_longer(cols = c(-site), names_to = "variable", values_to = "value") %>%
  group_nest(site) %>%
  mutate(stats = map(.x = data, .f =~ .x %>%
                       group_by(variable) %>%
                       summarize(n = n(), 
                                 mean = mean(value), 
                                 sd = sd(value), 
                                 min = min(value), 
                                 max = max(value),
                                 CV = sd/mean))) %>%
  select(-data) %>%
  unnest(cols = c(site, stats))

stats_by_station
```


Examine the hypothesis: Boxâ€™s M-test for Homogeneity of Covariance Matrices across groups
```{r}
epochdat.boxm <- data_clean_1 %>%
  select(site, sl:meandom_8) 

epochdat.boxm
```



Examine the variable correlation:
```{r}
cor <- data_clean_1 %>%
  dplyr::select(sl:meandom_8) %>%
  cor() %>%
  corrplot(method = "color") 
```


## Variables selection by greedy wilks - rank variables

A stepwise forward variable selection is performed. The initial model is defined by starting with the variable which separates the groups most. The model is then extended by including further variables depending on the Wilk's lambda criterion: Select the one which minimizes the Wilk's lambda of the model including the variable if its p-value still shows statistical significance. See [document](https://www.rdocumentation.org/packages/klaR/versions/1.7-0/topics/greedy.wilks) for more details:

The first eight variables are: sl, i2, i3, duration_8, meandom_6, duration_3, meandom_8, duration_2

```{r}
var_selection <- data_clean_1 %>% 
  preProcess(method = c("center", "scale")) %>%
  predict(data_clean_1) %>%
  greedy.wilks(individual ~ sl + i1 + i2 + i3 + min_d + max_d +
                 duration_1 + duration_2 + duration_3 + duration_4 + 
                 duration_5 + duration_6 + duration_7 + duration_8 +
                 dfrange_1 + dfrange_2 + dfrange_3 + dfrange_4 +
                 dfrange_5 + dfrange_6 + dfrange_7 + dfrange_8 +
                 meandom_1 + meandom_2 + meandom_3 + meandom_4 +
                 meandom_5 + meandom_6 + meandom_7 + meandom_8
               , data = .)

head(var_selection$results, 10)
```


## Linear discriminant analysis with n (=5) fold validation and m (=100) bootstrapping with p variables - determine p

Calculating the final confusion matrix and accuracy (mean & sd), kappa (mean & sd) through bootstrapping. Careful that it might take about 5 minutes to run this chunk:
```{r}
cm_p_1 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 1)
cm_p_2 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 2)
cm_p_3 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 3)
cm_p_4 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 4)
cm_p_5 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 5)
cm_p_6 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 6)
cm_p_7 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 7)
cm_p_8 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 8)
cm_p_9 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 9)
cm_p_10 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 10)
```

Plot out the result to decide the best number of variables that we want to use in the final model:
```{r}
std <- function(x) sd(x)/sqrt(length(x))

acc_kappa_p <- bind_cols(cm_p_1[[2]], cm_p_2[[2]], cm_p_3[[2]], cm_p_4[[2]], cm_p_5[[2]], 
                         cm_p_6[[2]], cm_p_7[[2]], cm_p_8[[2]], cm_p_9[[2]], cm_p_10[[2]],
                         cm_p_1[[3]], cm_p_2[[3]], cm_p_3[[3]], cm_p_4[[3]], cm_p_5[[3]], 
                         cm_p_6[[3]], cm_p_7[[3]], cm_p_8[[3]], cm_p_9[[3]], cm_p_10[[3]]) %>%
  rename(!!setNames(names(.), c(paste0("Accuracy_", seq(1:10)), paste0("Cohen's kappa_", seq(1:10))))) %>%
  pivot_longer(everything(), names_to = "method", values_to = "value") %>%
  separate(col = method, sep = "_", into = c("metrics", "n_variables")) %>%
  mutate(n_variables = factor(n_variables, levels = unique(n_variables))) %>%
  # ggplot(aes(x = n_variables, y = value)) +
  # geom_boxplot() +
  # geom_jitter(color = "steelblue",
  #             size = 0.9, 
  #             position = position_jitter(width = 0.15),
  #             alpha = 0.3) +
  # facet_wrap(~metrics, nrow = 1) +
  # theme_bw()
  group_by(metrics, n_variables) %>%
  summarize(mean_value = mean(value),
            std_value = std(value)) %>%
  ungroup() %>%
  ggplot(aes(x = n_variables, y = mean_value, group = metrics)) +
  geom_point(size = 5, shape = 18) +
  geom_errorbar(aes(ymin = mean_value - 1.96*std_value,
                    ymax = mean_value + 1.96*std_value),
                width = 0.3, size = 1.2) +
  geom_line(size = 1.2, aes(linetype = metrics), alpha = 0.5) +
  scale_colour_viridis_d() +
  xlab("Number of variables") +
  ylab("Value") +
  theme_bw() +
  theme(axis.title = element_text(size = 16),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(size = 14),
        legend.position = c(0.8, 0.2), 
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))

  
acc_kappa_p

ggsave(plot = acc_kappa_p, 
       filename = here("docs", "figures", "acc_kappa_p.png"), 
       width = 20, height = 12, units = "cm",
       dpi = 300)

```



## Final model visualization & evaluation

Here is the visualization of the best LDA model (select p variables):
```{r}
variables <- c("sl", "i2", "i3", "duration_8", "meandom_6", 
                 "duration_3", "meandom_8", "duration_2", "meandom_5", "meandom_1")

cbPalette <- c("#999999", "#000000", "#E69F00", "#56B4E9", "#D34623", "#009E73", "#CC79A7", "#F0E442", "#0072B2", "#D55E00")
sites <- data_clean_1 %>%
  transmute(site = str_sub(site, start = 4, end = 5)) %>%
  pull(site)

data_transformed <- data_clean_1 %>%
  preProcess(method = c("center", "scale")) %>%
  predict(data_clean_1) %>%
  select(all_of(c("individual", variables[1:6]))) 

model <- data_transformed %>%
  lda(individual ~ ., data = .)
  
# lda_data <- cbind(data_transformed, predict(model)$x)
# ggplot(lda_data, aes(LD1, LD2)) +
#   geom_point(aes(color = individual))

LD_plot <- ggord(model, sites, arrow = NULL, txt = NULL, size = 3) +
  coord_fixed(ratio = 0.7) +
  scale_fill_manual(values=cbPalette) +
  scale_colour_manual(values=cbPalette) +
  theme(legend.position = "right",
        legend.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))

LD_plot

ggsave(plot = LD_plot, 
       filename = here("docs", "figures", "LD_plot.png"), 
       height = 12,
       width = 20, 
       units = "cm", 
       dpi = 300)
```



And here is a visualization of the confusion matrix:
```{r}
df <- melt(cm_p_6[[1]]$table) %>%
  mutate(value = round(value),
         predicted = str_sub(predicted, start = 9, end = 10),
         observed = str_sub(observed, start = 9, end = 10)) 

confusion <- ggplot(df, aes(x = observed, y = predicted, fill = value)) +
  geom_tile(color = "gray90",
            lwd = 1.5,
            linetype = 1) +
  scale_fill_gradient2(high = "black", 
                       low = "white",
                       na.value = "white",
                       trans = "log",
                       labels = label_number(accuracy = 1)) +
  #scale_fill_gradient(low = "white", high = "black", na.value = "white") +
  coord_fixed() +
  labs(x = "Observed site",
       y = "Predicted site",
       fill = "# of calls") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))

confusion

```

```{r}
model_statistics <- read_csv(here("data", "final_model_statistics.csv")) 

stat_plot <- model_statistics %>%
  pivot_longer(cols = !...1, names_to = "site", values_to = "value") %>% 
  rename("metrics" = `...1`) %>%
  filter(metrics %in% c("Sensitivity", "Specificity")) %>%
  mutate(site = str_sub(site, start = 9, end = 10)) %>%
  ggplot(aes(x = site, y = value, fill = metrics)) + 
    geom_bar(stat = "identity", position = "dodge") +
    scale_fill_manual(values = c("aquamarine4", "darkolivegreen3")) +
    theme_bw() +
  labs(x = "Site",
       y = "Value",
       fill = "Metrics") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))


stat_plot


eval <- confusion + stat_plot +
  plot_layout(widths = c(1,1)) +
  plot_annotation(tag_levels = 'A') & 
  theme(plot.tag = element_text(size = 20))


ggsave(plot = eval, 
       filename = here("docs", "figures", "eval.png"),
       height = 12,
       width = 30, 
       units = "cm", 
       dpi = 300)
```


## Test the relationship between vocal similarity vs spatial distance

```{r}
vocal_spatial <- cbind(data_transformed, predict(model)$x) %>%
  group_by(individual) %>%
  summarize(LD1_mean = mean(LD1),
            LD2_mean = mean(LD2)) %>%
  mutate(individual = str_sub(individual, start = 6)) %>%
  left_join(data_location_clean, by = c("individual" = "site"))

coordinates <- cbind(vocal_spatial$longitude, vocal_spatial$latitude)
sites_sp <- vect(coordinates, crs = "+proj=longlat +datum=WGS84")


dist_spatial <- vocal_spatial %>%
  select(longitude, latitude) %>%
  as.matrix() %>%
  vect(crs = "+proj=longlat +datum=WGS84") %>%
  terra::distance()
  
dist_vocal <- vocal_spatial %>%
  select(LD1_mean, LD2_mean) %>%
  dist(method = "euclidean")


vocal_spatial_pair <- tibble(dist_vocal = dist_vocal,
                             dist_spatial = dist_spatial * 0.001)

similarity <- ggplot(aes(x = dist_spatial, y = dist_vocal), data = vocal_spatial_pair) +
  geom_point(shape = 18, size = 4, colour = "black") +
  geom_smooth(method = "lm", colour = "black") +
  stat_cor(method = "pearson", label.x = 0.2, label.y = 8.5, size = 5) +
  theme_bw() +
  labs(x = "Distance between sites (km)",
       y = "Vocal similarity") +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)))

ggsave(plot = similarity,
       filename = here("docs", "figures", "similarity.png"),
       height = 12,
       width = 20, 
       units = "cm", 
       dpi = 300)

```





