---
title: "Barred Owl Individual Vocalization using Passive Acoustic Monitoring"
author: "Sunny Tseng"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Library & functions

Load necessary libraries: 
```{r, message = FALSE, warning = FALSE}
library(here)
library(tidyverse)
library(lubridate)
library(reshape2)
library(sf)
library(leaflet)
library(ggmap)
library(ggspatial)

library(factoextra)
library(ggbiplot)
library(corrplot)
library(psych)
library(caret)
library(MASS)
library(klaR)
```


Also do this to avoid name conflict:
```{r}
here <- here::here
select <- dplyr::select
filter <- dplyr::filter
summarize <- dplyr::summarize
group_by <- dplyr::group_by
rename <- dplyr::rename
```


Load personal functions:
```{r}
source(here("R", "Ch2_1_qualified_recording_list.R"))
source(here("R", "Ch2_2_move_recordings_to_folder.R"))
source(here("R", "Ch2_3_parameters_from_raventable.R"))
source(here("R", "Ch2_4_lda_n_fold_validation.R"))
```


## Data import & data cleaning

The original output from BirdNET, filtered by species == Barred Owl: 
```{r, message = FALSE}
data_birdnet <- read_csv(here("data", "barred_owl_recordings_list.csv"))

data_birdnet_clean <- data_birdnet %>%
  filter(year == 2021) %>%
  drop_na() %>%
  mutate(date = ymd(paste0(year, month, day)),
         hour = str_sub(recording, start = 10, end = 11),
         Julian = yday(date), 
         day_period = cut(Julian, 
                          breaks = c(40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150)))
```


The song dataset after manual extraction of features. Only keep the sites with more than 9 calls:
```{r, message = FALSE}
data <- read_csv(here("data", "barred_owl_recordings_list_parameters_2.csv"))

data_clean <- data %>%
  filter_all(all_vars(. != 0)) %>%
  mutate(site = str_sub(sound.files, start = 1, end = 5),
         year = str_sub(sound.files, start = 7, end = 10),
         month = str_sub(sound.files, start = 11, end = 12),
         day = str_sub(sound.files, start = 13, end = 14),
         unique_ID = seq(from = 1, to = nrow(.)),
         individual = paste0(year, "_", site),
         date = ymd(paste0(year, month, day)))

sites <- data_clean %>%
  group_by(site) %>%
  summarize(n = n()) %>%
  filter(n >= 9) %>%
  pull(site)

data_clean_1 <- data_clean %>%
  filter(site %in% sites)
```


Location of the 66 ARU sites:
```{r, message = FALSE}
data_location <- read_csv(here("data", "JPRF_all_sites.csv"))

data_location_clean <- data_location %>%
  rename("site" = Name, "longitude" = X, "latitude" = Y) %>%
  select(site, longitude, latitude) %>%
  mutate(group = str_extract(site, pattern = ".+(?=\\_)"))
```


## Data visualization

Temporal distribution of the Barred Owl calls, after filtering out sites with fewer than 9 calls: 
```{r, message = FALSE}
temporal_songs <- data_clean_1 %>%
  group_by(date, site) %>%
  summarize(n_songs = n()) %>%
  ungroup() %>%
  ggplot(aes(x = date, y = site, fill = n_songs)) +
    geom_point(shape = 22, size = 7) +
    scale_fill_gradient(low = "mistyrose", high = "red") +
    theme_bw() +
    theme(legend.position = c(0.99, 0.05), 
          legend.justification = c("right", "bottom"),
          legend.box.just = "right",
          axis.title = element_text(size = 14)) +
    labs(fill = "# of calls", x = "Date", y = "Site",
         title = "# of BDOW calls per site per day")

temporal_songs
```


Spatial distribution of the ARU sites and Barred Owl calls:
```{r}
JPRF_sf <- full_join(data_location_clean, data_clean_1 %>% group_by(site) %>% summarize(n_songs = n()),
                     by = "site", replace_na) %>%
  replace_na(list(n_songs = 0)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  ggplot(data = .) +
    geom_sf(aes(colour = group), shape = 3, stroke = 1.5, size = 3) +
    geom_sf(aes(size = n_songs), shape = 1, stroke = 2, data = . %>% filter(n_songs != 0)) +
    annotation_scale(location = "bl", width_hint = 0.4) +
    annotation_north_arrow(location = "bl", which_north = "true",
                           pad_x = unit(0.2, "in"), pad_y = unit(0.3, "in"),
                           style = north_arrow_fancy_orienteering) +
    coord_sf(xlim = c(-124.7, -124.1), ylim = c(54.55, 54.85)) +
    theme_bw()

JPRF_sf

```


Spatial distribution of the ARU sites using package leavlet:
```{r, eval = FALSE}
field_sites <- JPRF_sf %>%
  rename("site_name" = site)

leaflet() %>%
  setView(lng = -123.5, lat = 52, zoom = 10) %>%
  addProviderTiles("Stamen") %>%
  addCircleMarkers(
    data = field_sites,
    radius = 5,
    color = "black",
    stroke = FALSE,
    fillOpacity = 0.8,
    popup = paste("Site name: ", field_sites$site_name)
  )
```


## Predictor variables 

Examine the variables performance by looking at the CV:
```{r}
statistics <- data_clean_1 %>%
  select(site, sl:meandom_8) %>%
  pivot_longer(cols = c(-site), names_to = "variable", values_to = "value") %>%
  group_nest(variable) %>%
  mutate(mean = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% mean()),
         sd = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% sd()),
         min = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% min()),
         max = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% max()),
         CV_group = map_dbl(.x = data, .f =~ sd(.x$value)/mean(.x$value)*100),
         CV_individual = map_dbl(.x = data, .f =~ .x %>% 
                                   group_by(site) %>%
                                   dplyr::summarise(CV = sd(value)/mean(value)) %>%
                                   pull(CV) %>%
                                   mean() *100),
         ANOVA = map_dbl(.x = data, .f =~ .x %>%
                           lm(formula = value ~ site) %>%
                           anova() %>%
                           .$`Pr(>F)` %>%
                           .[1])) %>%
  select(-data)

statistics
```


Examine the variable correlation:
```{r}
cor <- data_clean_1 %>%
  dplyr::select(sl:meandom_8) %>%
  cor() %>%
  corrplot(method = "color") 
```


## Variables selection by greedy wilks - rank variables

A stepwise forward variable selection is performed. The initial model is defined by starting with the variable which separates the groups most. The model is then extended by including further variables depending on the Wilk's lambda criterion: Select the one which minimizes the Wilk's lambda of the model including the variable if its p-value still shows statistical significance. See [document](https://www.rdocumentation.org/packages/klaR/versions/1.7-0/topics/greedy.wilks) for more details:

The first eight variables are: sl, i2, i3, duration_8, meandom_6, duration_3, meandom_8, duration_2

```{r}
var_selection <- data_clean_1 %>% 
  preProcess(method = c("center", "scale")) %>%
  predict(data_clean_1) %>%
  greedy.wilks(individual ~ sl + i1 + i2 + i3 + min_d + max_d +
                 duration_1 + duration_2 + duration_3 + duration_4 + 
                 duration_5 + duration_6 + duration_7 + duration_8 +
                 dfrange_1 + dfrange_2 + dfrange_3 + dfrange_4 +
                 dfrange_5 + dfrange_6 + dfrange_7 + dfrange_8 +
                 meandom_1 + meandom_2 + meandom_3 + meandom_4 +
                 meandom_5 + meandom_6 + meandom_7 + meandom_8
               , data = .)

head(var_selection$results, 10)
```


## Linear discriminant analysis with n (=5) fold validation and m (=100) bootstrapping with p variables - determine p

Calculating the final confusion matrix and accuracy (mean & sd), kappa (mean & sd) through bootstrapping. Careful that it might take about 5 minutes to run this chunk:
```{r}
cm_p_1 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 1)
cm_p_2 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 2)
cm_p_3 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 3)
cm_p_4 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 4)
cm_p_5 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 5)
cm_p_6 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 6)
cm_p_7 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 7)
cm_p_8 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 8)
cm_p_9 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 9)
cm_p_10 <- lda_n_fold_validation(dataset = data_clean_1, n = 5, m = 100, p = 10)
```

Plot out the result to decide the best number of variables that we want to use in the final model:
```{r}
std <- function(x) sd(x)/sqrt(length(x))

acc_kappa_p <- bind_cols(cm_p_1[[2]], cm_p_2[[2]], cm_p_3[[2]], cm_p_4[[2]], cm_p_5[[2]], 
                         cm_p_6[[2]], cm_p_7[[2]], cm_p_8[[2]], cm_p_9[[2]], cm_p_10[[2]],
                         cm_p_1[[3]], cm_p_2[[3]], cm_p_3[[3]], cm_p_4[[3]], cm_p_5[[3]], 
                         cm_p_6[[3]], cm_p_7[[3]], cm_p_8[[3]], cm_p_9[[3]], cm_p_10[[3]]) %>%
  rename(!!setNames(names(.), c(paste0("Accuracy_", seq(1:10)), paste0("Cohen's kappa_", seq(1:10))))) %>%
  pivot_longer(everything(), names_to = "method", values_to = "value") %>%
  separate(col = method, sep = "_", into = c("metrics", "n_variables")) %>%
  mutate(n_variables = factor(n_variables, levels = unique(n_variables))) %>%
  # ggplot(aes(x = n_variables, y = value)) +
  # geom_boxplot() +
  # geom_jitter(color = "steelblue",
  #             size = 0.9, 
  #             position = position_jitter(width = 0.15),
  #             alpha = 0.3) +
  # facet_wrap(~metrics, nrow = 1) +
  # theme_bw()
  group_by(metrics, n_variables) %>%
  summarize(mean_value = mean(value),
            std_value = std(value)) %>%
  ungroup() %>%
  ggplot(aes(x = n_variables, y = mean_value,
             colour = metrics, group = metrics)) +
    geom_line(size = 1.2) +
    geom_point(size = 5, shape = 1) +
    geom_errorbar(aes(ymin = mean_value - 1.96*std_value,
                      ymax = mean_value + 1.96*std_value),
                  width = 0.2,
                  size = 1.2) +
    scale_colour_viridis_d() +
    xlab("Number of variables") +
    ylab("Value") +
    theme_bw() +
    theme(axis.title = element_text(size = 16),
          axis.text = element_text(size = 12),
          legend.title = element_blank(),
          legend.text = element_text(size = 14),
          legend.position = c(0.8, 0.2))
  
acc_kappa_p
```



## Final model visualization & evaluation

Here is the visualization of the best LDA model (select p variables):
```{r}
variables <- c("sl", "i2", "i3", "duration_8", "meandom_6", 
                 "duration_3", "meandom_8", "duration_2", "meandom_5", "meandom_1")

data_transformed <- data_clean_1 %>%
  preProcess(method = c("center", "scale")) %>%
  predict(data_clean_1) %>%
  select(all_of(c("individual", variables[1:6]))) 

model <- data_transformed %>%
  lda(individual ~ ., data = .)
  
# lda_data <- cbind(data_transformed, predict(model)$x)
# ggplot(lda_data, aes(LD1, LD2)) +
#   geom_point(aes(color = individual))

ggord(model, data_clean_1$site, arrow = NULL, txt = NULL) +
  coord_fixed(ratio = 0.7)
```



And here is a visualization of the confusion matrix:
```{r}
df <- melt(cm_p_6[[1]]$table)
colnames(df) <- c("x", "y", "value")

ggplot(df, aes(x = x, y = y, fill = value %>% log())) +
  geom_tile(color = "white",
            lwd = 1.5,
            linetype = 1) +
  coord_fixed() +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.1, hjust = 0.2))

```












