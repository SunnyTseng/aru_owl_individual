---
title: "Barred Owl Individual Vocalization using Passive Acoustic Monitoring"
author: "Sunny Tseng"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Library

Load necessary libraries: 
```{r, message = FALSE, warning = FALSE}
library(here)
library(tidyverse)
library(lubridate)
library(sf)
library(leaflet)
library(ggmap)
library(ggspatial)

library(factoextra)
library(ggbiplot)
library(corrplot)
library(psych)
library(caret)
library(MASS)
library(klaR)
```


Also do this to avoid name conflict:
```{r}
here <- here::here
select <- dplyr::select
filter <- dplyr::filter
summarize <- dplyr::summarize
group_by <- dplyr::group_by
rename <- dplyr::rename
```


Load personal functions:
```{r}
source(here("R", "Ch2_1_qualified_recording_list.R"))
source(here("R", "Ch2_2_move_recordings_to_folder.R"))
source(here("R", "Ch2_3_parameters_from_raventable.R"))
source(here("R", "Ch2_4_lda_n_fold_validation.R"))
```


## Data import & data cleaning

The original output from BirdNET, filtered by species == Barred Owl: 
```{r, message = FALSE}
data_birdnet <- read_csv(here("data", "barred_owl_recordings_list.csv"))

data_birdnet_clean <- data_birdnet %>%
  filter(year == 2021) %>%
  drop_na() %>%
  mutate(date = ymd(paste0(year, month, day)),
         hour = str_sub(recording, start = 10, end = 11),
         Julian = yday(date), 
         day_period = cut(Julian, 
                          breaks = c(40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150)))
```


The song dataset after manual extraction of features. Only keep the sites with more than 9 calls:
```{r, message = FALSE}
data <- read_csv(here("data", "barred_owl_recordings_list_parameters_2.csv"))

data_clean <- data %>%
  filter_all(all_vars(. != 0)) %>%
  mutate(site = str_sub(sound.files, start = 1, end = 5),
         year = str_sub(sound.files, start = 7, end = 10),
         month = str_sub(sound.files, start = 11, end = 12),
         day = str_sub(sound.files, start = 13, end = 14),
         unique_ID = seq(from = 1, to = nrow(.)),
         individual = paste0(year, "_", site),
         date = ymd(paste0(year, month, day)))

sites <- data_clean %>%
  group_by(site) %>%
  summarize(n = n()) %>%
  filter(n >= 9) %>%
  pull(site)

data_clean_1 <- data_clean %>%
  filter(site %in% sites)
```


Location of the 66 ARU sites:
```{r, message = FALSE}
data_location <- read_csv(here("data", "JPRF_all_sites.csv"))

data_location_clean <- data_location %>%
  rename("site" = Name, "longitude" = X, "latitude" = Y) %>%
  select(site, longitude, latitude) %>%
  mutate(group = str_extract(site, pattern = ".+(?=\\_)"))
```


## Data visualization

Temporal distribution of the Barred Owl calls, after filtering out sites with fewer than 9 calls: 
```{r, message = FALSE}
temporal_songs <- data_clean_1 %>%
  group_by(date, site) %>%
  summarize(n_songs = n()) %>%
  ungroup() %>%
  ggplot(aes(x = date, y = site, fill = n_songs)) +
    geom_point(shape = 22, size = 7) +
    scale_fill_gradient(low = "mistyrose", high = "red") +
    theme_bw() +
    theme(legend.position = c(0.99, 0.05), 
          legend.justification = c("right", "bottom"),
          legend.box.just = "right",
          axis.title = element_text(size = 14)) +
    labs(fill = "# of calls", x = "Date", y = "Site",
         title = "# of BDOW calls per site per day")

temporal_songs
```


Spatial distribution of the ARU sites and Barred Owl calls:
```{r}
JPRF_sf <- full_join(data_location_clean, data_clean_1 %>% group_by(site) %>% summarize(n_songs = n()),
                     by = "site", replace_na) %>%
  replace_na(list(n_songs = 0)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  ggplot(data = .) +
    geom_sf(aes(colour = group), shape = 3, stroke = 1.5, size = 3) +
    geom_sf(aes(size = n_songs), shape = 1, stroke = 2, data = . %>% filter(n_songs != 0)) +
    annotation_scale(location = "bl", width_hint = 0.4) +
    annotation_north_arrow(location = "bl", which_north = "true",
                           pad_x = unit(0.2, "in"), pad_y = unit(0.3, "in"),
                           style = north_arrow_fancy_orienteering) +
    coord_sf(xlim = c(-124.7, -124.1), ylim = c(54.55, 54.85)) +
    theme_bw()

JPRF_sf

```


Spatial distribution of the ARU sites using package leavlet:
```{r, eval = FALSE}
field_sites <- JPRF_sf %>%
  rename("site_name" = site)

leaflet() %>%
  setView(lng = -123.5, lat = 52, zoom = 10) %>%
  addProviderTiles("Stamen") %>%
  addCircleMarkers(
    data = field_sites,
    radius = 5,
    color = "black",
    stroke = FALSE,
    fillOpacity = 0.8,
    popup = paste("Site name: ", field_sites$site_name)
  )
```


## Predictor variables 

Examine the variables performance by looking at the CV:
```{r}
statistics <- data_clean_1 %>%
  select(site, sl:meandom_8) %>%
  pivot_longer(cols = c(-site), names_to = "variable", values_to = "value") %>%
  group_nest(variable) %>%
  mutate(mean = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% mean()),
         sd = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% sd()),
         min = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% min()),
         max = map_dbl(.x = data, .f =~ .x %>% pull(value) %>% max()),
         CV_group = map_dbl(.x = data, .f =~ sd(.x$value)/mean(.x$value)*100),
         CV_individual = map_dbl(.x = data, .f =~ .x %>% 
                                   group_by(site) %>%
                                   dplyr::summarise(CV = sd(value)/mean(value)) %>%
                                   pull(CV) %>%
                                   mean() *100),
         ANOVA = map_dbl(.x = data, .f =~ .x %>%
                           lm(formula = value ~ site) %>%
                           anova() %>%
                           .$`Pr(>F)` %>%
                           .[1])) %>%
  select(-data)

statistics
```


Examine the variable correlation:
```{r}
cor <- data_clean_1 %>%
  dplyr::select(sl:meandom_8) %>%
  cor() %>%
  corrplot(method = "color") 
```


## Variables selection by greedy wilks: 

A stepwise forward variable selection is performed. The initial model is defined by starting with the variable which separates the groups most. The model is then extended by including further variables depending on the Wilk's lambda criterion: Select the one which minimizes the Wilk's lambda of the model including the variable if its p-value still shows statistical significance. See [document](https://www.rdocumentation.org/packages/klaR/versions/1.7-0/topics/greedy.wilks) for more details:

The first eight variables are: sl, i2, i3, duration_8, meandom_6, duration_3, meandom_8, duration_2

```{r}
train_transformed <- data_clean_1 %>% 
  preProcess(method = c("center", "scale")) %>%
  predict(data_clean_1) %>%
  greedy.wilks(individual ~ sl + i1 + i2 + i3 + min_d + max_d +
                 duration_1 + duration_2 + duration_3 + duration_4 + 
                 duration_5 + duration_6 + duration_7 + duration_8 +
                 dfrange_1 + dfrange_2 + dfrange_3 + dfrange_4 +
                 dfrange_5 + dfrange_6 + dfrange_7 + dfrange_8 +
                 meandom_1 + meandom_2 + meandom_3 + meandom_4 +
                 meandom_5 + meandom_6 + meandom_7 + meandom_8
               , data = .)

```


## Analysis: linear discriminant analysis with 5 fold validation

```{r}
set.seed(110)

table_all <- lda_n_fold_validation(dataset = data_clean_1, n = 5)

table_all_cm <- confusionMatrix(table_all)
accuracy <- table_all_cm$overall[1]
kappa <- table_all_cm$overall[2]

accuracy
kappa

```














